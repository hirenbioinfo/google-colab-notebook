{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPWlIHeR49ZGJrqUrxWepr/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hirenbioinfo/google-colab-notebook/blob/main/colab_pyspark_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Install Pyspark\n",
        "\n",
        "Installing PySpark is a straightforward process, but it requires some pre-requisites. PySpark is the Python library for Apache Spark, a fast and general-purpose cluster-computing framework for big data processing."
      ],
      "metadata": {
        "id": "SOen6kEdg1Xr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Here are the steps to install PySpark for Python from source\n",
        "\n",
        "#Install Java\n",
        "#!apt-get update\n",
        "#!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "\n",
        "# Download and install Spark\n",
        "#!wget -q http://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\n",
        "#!tar xf spark-3.2.1-bin-hadoop3.2.tgz\n",
        "\n",
        "# Install PySpark\n",
        "!pip install -q pyspark\n",
        "\n",
        "# Set Environment Variables\n",
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.2.1-bin-hadoop3.2\"\n",
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql.import Sparksession\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "#spark\n",
        "\n",
        "# Start SparkSession\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.master(\"local[*]\").getOrCreate()\n"
      ],
      "metadata": {
        "id": "hDrG-4pQwZw3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#optional\n",
        "#on cluster\n",
        "from pyspark import SparkContext\n",
        "from pyspark.streaming import StreamingContext\n",
        "\n",
        "# Create a local StreamingContext with two working thread and batch interval of 1 second\n",
        "sc = SparkContext(\"local[2]\", \"NetworkWordCount\")\n",
        "ssc = StreamingContext(sc, 1)\n",
        "\n",
        "# Create a DStream that will connect to hostname:port, like localhost:9999\n",
        "lines = ssc.socketTextStream(\"localhost\", 9999)"
      ],
      "metadata": {
        "id": "QPwEBiISgfqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# on google colab\n",
        "! pip install pyspark py4j #all you need"
      ],
      "metadata": {
        "id": "_pbqkWwQugdS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# intitiate a spark season\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName('spark-dataframe-demo').getOrCreate()\n",
        "# see spark version\n",
        "spark"
      ],
      "metadata": {
        "id": "g5CR27OXulOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#import requests\n",
        "#import pandas as pd\n",
        "#from pyspark.sql import SparkSession\n",
        "#from pyspark.sql import DataFrame"
      ],
      "metadata": {
        "id": "mXJvZztHqBTU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Mount Google Drive:\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "QQahefG1sX0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Set Up PySpark:\n",
        "#If you haven't set up PySpark yet, do the steps mentioned previously:\n",
        "\n",
        "# #!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "# #!wget -q https://archive.apache.org/dist/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz\n",
        "# # !tar xf spark-3.1.2-bin-hadoop3.2.tgz\n",
        "# !pip install -q findspark\n",
        "\n",
        "# import os\n",
        "# os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "# os.environ[\"SPARK_HOME\"] = \"/content/spark-3.1.2-bin-hadoop3.2\"\n",
        "\n",
        "# import findspark\n",
        "# findspark.init()\n"
      ],
      "metadata": {
        "id": "YlG_D3bBwjBP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Read the CSV File from Google Drive with PySpark:\n",
        "#from pyspark.sql import SparkSession\n",
        "#spark = SparkSession.builder\\\n",
        "#   .appName('GoogleDriveCSV')\\\n",
        "#  .getOrCreate()\n"
      ],
      "metadata": {
        "id": "vw3D46-AwjKz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read file from goolge drive\n",
        "# we have manually downaloded the origianl csv file and saved to google drive\n",
        "file_path = '/content/drive/My Drive/Udemy/original.csv'\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "# see the dataframe\n",
        "#by default, the show() method displays 20 rows\n",
        "df.show()\n",
        "# first five row of the dataframe\n",
        "df.show(5)"
      ],
      "metadata": {
        "id": "Vuzjtg56wjRU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TzVMmXSGmvq6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "e-gFR-MtpSAe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.types import *\n",
        "\n",
        "schema = StructType([\n",
        "    StructField('id', IntegerType(), True),\n",
        "    StructField('first_name', StringType(), True),\n",
        "    StructField('last_name', StringType(), True),\n",
        "    StructField('gender', StringType(), True),\n",
        "    StructField('City', StringType(), True),\n",
        "    StructField('JobTitle', StringType(), True),\n",
        "    StructField('Salary', StringType(), True),\n",
        "    StructField('Latitude', StringType(), True),\n",
        "    StructField('Longitude', FloatType(), True),\n",
        "])"
      ],
      "metadata": {
        "id": "1Lw4HS4NqMzc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 =spark.read.csv(file_path, header=True, schema=schema)\n",
        "df2.show()"
      ],
      "metadata": {
        "id": "KiSLaGH0s3Ph"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.printSchema()"
      ],
      "metadata": {
        "id": "fhNG62kvznEL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# describe the dataframe\n",
        "df2.describe().show()\n",
        "#similar in pandas\n",
        "#print(df.describe())"
      ],
      "metadata": {
        "id": "KH-y6qb-ro-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# list of the columns\n",
        "df2.columns\n",
        "df2.distinct().count()"
      ],
      "metadata": {
        "id": "tJI5XaHptDu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "distinct_count_gender = df2.select(\"City\").distinct().count()\n",
        "print(distinct_count_gender)"
      ],
      "metadata": {
        "id": "FhE7SNjgt1kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total number of unique rows\n",
        "print(\"The number of unique values in the 'gender' column is: {}\".format(distinct_count_gender))"
      ],
      "metadata": {
        "id": "A7LQpXAWt8w_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Count null values for each column\n",
        "from pyspark.sql.functions import when, count, isnull, col\n",
        "null_counts = df.agg(*[count(when(isnull(c), c)).alias(c) for c in df.columns])\n",
        "null_counts.show()\n",
        "# for a particular column\n",
        "null_cities = df.filter(df[\"City\"].isNull()).count()\n",
        "null_job_titles = df.filter(df[\"JobTitle\"].isNull()).count()\n",
        "print(f\"Entries with null cities: {null_cities}\")\n",
        "print(f\"Entries with null job titles: {null_job_titles}\")"
      ],
      "metadata": {
        "id": "A-sGuKb9uwAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handle the null values\n",
        "df2_dropped=df2.na.drop()\n",
        "df2_dropped.show()"
      ],
      "metadata": {
        "id": "4Ntag90Mu5cB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# handle null to a specif column\n",
        "# see which column jobTitle is null\n",
        "df2_null_jobs=df2.filter(df2.JobTitle.isNull())\n",
        "df2_null_jobs.show()\n",
        "\n",
        "# see which column jobTitle is not null\n",
        "df2_null_jobs=df2.filter(df2.JobTitle.isNotNull())\n",
        "df2_null_jobs.show()"
      ],
      "metadata": {
        "id": "niFNT-q9vmke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# see the na rows\n",
        "df2.select(df2.JobTitle.isNull().alias(\"isJobTitleNull\")).show()"
      ],
      "metadata": {
        "id": "GOTxVd2awxOK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering\n",
        "df2.filter(\"gender == 'Male'\").show()\n",
        "df2.where(\"gender == 'Female'\").show()"
      ],
      "metadata": {
        "id": "6cOlgoJi3H4T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering with multiple condition\n",
        "df2.filter(df2.City == \"Bulgan\").show()\n",
        "df2.filter((df2[\"City\"] == \"Bulgan\") & (df2[\"gender\"] == \"Female\")).show()"
      ],
      "metadata": {
        "id": "aOtaTz4530kM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "\n",
        "# Replace anything that's not a letter, a number, or a space with an empty string in the 'City' column\n",
        "df_cleaned = df.withColumn(\"City\", regexp_replace(df[\"City\"], \"[^a-zA-Z0-9 ]\", \"\"))\n",
        "\n",
        "df_cleaned.show()"
      ],
      "metadata": {
        "id": "YhtvTpIqzCqg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Sq6yjGjrupHX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import regexp_replace\n",
        "import matplotlib.pyplot as plt\n",
        "df_cleaned_salary = df.withColumn(\"Cleaned_Salary\", regexp_replace(df[\"Salary\"], \"[$,]\", \"\").cast(\"float\"))\n",
        "salary_data = df_cleaned_salary.select(\"Cleaned_Salary\").rdd.flatMap(lambda x: x).collect()\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.hist(salary_data, bins=20, color='blue', edgecolor='black')\n",
        "plt.title('Salary Distribution')\n",
        "plt.xlabel('Salary')\n",
        "plt.ylabel('Number of Employees')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X6RGMGby8CXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "sns.kdeplot(salary_data, shade=True, color='blue')\n",
        "plt.title('Salary Density Plot')\n",
        "plt.xlabel('Salary')\n",
        "plt.ylabel('Density')\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IoZlqi_R8hAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "colors = ['blue' if gender == 'Male' else 'pink' for gender in genders]\n",
        "gender_counts = df.groupBy(\"gender\").count().collect()\n",
        "genders = [row['gender'] for row in gender_counts]\n",
        "counts = [row['count'] for row in gender_counts]\n",
        "import matplotlib.pyplot as plt\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.bar(genders, counts, color=colors)\n",
        "plt.title('Gender Distribution')\n",
        "plt.xlabel('Gender')\n",
        "plt.ylabel('Number of Individuals')\n",
        "plt.grid(axis='y')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ecJBZw5G8znI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define colors for each gender. Adjust this if you have more categories.\n",
        "colors = ['blue' if gender == 'Male' else 'pink' for gender in genders]\n",
        "\n",
        "plt.figure(figsize=(8,8))\n",
        "plt.pie(counts, labels=genders, colors=colors, autopct='%1.1f%%', startangle=140)\n",
        "plt.title('Gender Distribution')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "fm4wJIr79kzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.groupBy('City').agg(countDistinct('gender').alias('country_count')).orderBy(desc('country_count')).show()"
      ],
      "metadata": {
        "id": "PclYWUK3B6Vx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"Colab PySpark Session\") \\\n",
        "    .master(\"local[*]\") \\\n",
        "    .getOrCreate()\n"
      ],
      "metadata": {
        "id": "_xGWTWeHuqOR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"//california_housing_test.csv\"\n",
        "\n",
        "# Read the CSV file\n",
        "df = spark.read.csv(file_path, header=True, inferSchema=True)\n",
        "\n",
        "# Show the first few rows of the dataframe\n",
        "df.show()\n"
      ],
      "metadata": {
        "id": "NY7sAjxKw4aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "fJd08jOF0qtf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(15)"
      ],
      "metadata": {
        "id": "4nkSPkJX1Cr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_classification = df.withColumn(\"house_price_category\",\n",
        "                                       when(df[\"median_house_value\"] > 25000, \"costly\").otherwise(\"cheap\"))"
      ],
      "metadata": {
        "id": "LmRv-hXWsE6H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe.show()"
      ],
      "metadata": {
        "id": "3XfEC1qV2LtR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.describe().show()"
      ],
      "metadata": {
        "id": "uQP0UZcN2ayo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "df_cleaned = df.withColumn(\"Clean_City\", when(df.City.isNull(),'unknown').otherwise(df.City))\n",
        "df_cleaned = df.withColumn(\"New_JobTitle\", when(df.JobTitle.isNull(),'unknown').otherwise(df.JobTitle))"
      ],
      "metadata": {
        "id": "QbZCATyts08u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# same as above\n",
        "#from pyspark.sql.functions import *\n",
        "#df_cleaned = df.withColumn(\"Clean_City\", when(isnull(df[\"City\"]), 'unknown').otherwise(df[\"City\"]))"
      ],
      "metadata": {
        "id": "imaWKnpv4p0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_cleaned.show()"
      ],
      "metadata": {
        "id": "qMPURTCusQ53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates=df.dropDuplicates()"
      ],
      "metadata": {
        "id": "WsP01YeQ6kf5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_no_duplicates.show()"
      ],
      "metadata": {
        "id": "1nXc_fl0xipf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean=df.groupBy().avg('population').take(1)[0][0]"
      ],
      "metadata": {
        "id": "MSXcFy5CvjF9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_with_classification = df.withColumn(\"house_price_category\",\n",
        "                                       when(df[\"median_house_value\"] > 25000, \"costly\").otherwise(\"cheap\"))"
      ],
      "metadata": {
        "id": "LhikbQRxs2Ya"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mean.show()"
      ],
      "metadata": {
        "id": "FX21b_PPvsrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(mean)"
      ],
      "metadata": {
        "id": "rccEOMYHwJ3S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "A5TAj75Rxrxi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import lit"
      ],
      "metadata": {
        "id": "ymfldg9nwdHy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "test=df.select('latitude')"
      ],
      "metadata": {
        "id": "fmgLfBXU8nmF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test.show()"
      ],
      "metadata": {
        "id": "7BN1jqqY87Cs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import *\n",
        "mydf2="
      ],
      "metadata": {
        "id": "JCDwh4RpqeL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Na2aheuq9HT0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking data entries for each column\n",
        "df.select(['longitude',\n",
        " 'latitude',\n",
        " 'housing_median_age',\n",
        " 'total_rooms',\n",
        " 'total_bedrooms']).describe().show()"
      ],
      "metadata": {
        "id": "9FrBqsNAxxbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.select(['population',\n",
        " 'households',\n",
        " 'median_income',\n",
        " 'median_house_value']).describe().show()"
      ],
      "metadata": {
        "id": "qtPXIx_qx7if"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Checking if the prices are normally distributed\n",
        "sns.distplot(df.select('median_income').toPandas(), color=\"skyblue\")\n",
        "df.select(F.skewness('median_income'), F.kurtosis('price')).show()"
      ],
      "metadata": {
        "id": "3wm1jkneyRMm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "dt274L1OyWs5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install Seaborn"
      ],
      "metadata": {
        "id": "ssxeOP3eyaR-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install pyspark py4j #all you need\n",
        "from pyspark.sql import SparkSession\n",
        "from pyspark.sql.functions import concat\n",
        "\n",
        "# Initialize a Spark session\n",
        "spark = SparkSession.builder.appName(\"ConcatenateColumns\").getOrCreate()\n",
        "\n",
        "# Sample data\n",
        "data = [(\"John\", \"Doe\"), (\"Jane\", \"Smith\")]\n",
        "df = spark.createDataFrame(data, [\"first_name\", \"last_name\"])\n",
        "\n",
        "# Concatenate two columns\n",
        "df_with_fullname = df.withColumn(\"full_name\", concat(df[\"first_name\"], df[\"last_name\"]))\n",
        "\n",
        "df_with_fullname.show()\n"
      ],
      "metadata": {
        "id": "z0cO5RVFymig"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}